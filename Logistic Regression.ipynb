{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this excercise, we will be building a logistic regression with 2 features space. Recall that our logistic regression model is:\n",
    "\n",
    "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}$. Also, since this is a two-dimensional problem, we define $\\mathbf{w}^{\\top} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ and here, $\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\top}$, and $x_{i,0} \\triangleq 1$\n",
    "\n",
    "We interpret our logistic regression classifier output (or confidence score) as the conditional probability that the target variable for a given sample $y_i$ is from class \"1\", given the observed features, $\\mathbf{x}_i$. For one sample, $(y_i, \\mathbf{x}_i)$, this is given as:\n",
    "\n",
    "$$P(Y=1|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "In the context of maximizing the likelihood of our parameters given the data, we define this to be the likelihood function $L(\\mathbf{w}|y_i,\\mathbf{x}_i)$, corresponding to one sample observation from the training dataset.\n",
    "\n",
    "\n",
    "To simplify notation, please use $\\mathbf{w}^{\\top}\\mathbf{x}$ instead of writing out $w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$. Lastly, this will be a function of the features, $x_{i,j}$ (with the first index in the subscript representing the observation and the second the feature; targets, $y_i$; and the logistic regression model parameters, $w_j$.\n",
    "\n",
    "We first define conditional probabilities of observing each class given $\\mathbf{x}_{i}$ set of features:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P(y_{i}=1|\\mathbf{x}_{i}) = \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i}) \\\\\n",
    "P(y_{i}=0|\\mathbf{x}_{i}) = 1 - \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})\n",
    "\\end{aligned}$\n",
    "\n",
    "Using these probablities, we can define likelihood of an observation $i$ as follows:\n",
    "\n",
    "$\\begin{aligned}\n",
    "L(\\mathbf{w}|\\mathbf{y}_{i}, \\mathbf{x}_{i}) = \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})^{\\mathbf{y}_{i}}[1 - \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})]^{1-\\mathbf{y}_{i}}\n",
    "\\end{aligned}$\n",
    "\n",
    "In logistic regression, we make the independence assumption. Hence, we can write the likelihood function of all points (entire dataset) as:\n",
    "\n",
    "$\\begin{aligned}\n",
    "L(\\mathbf{w}|\\mathbf{y}, \\mathbf{X}) = \\prod_{n=i}^{N} {\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})^{\\mathbf{y}_{i}}[1 - \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})]^{1-\\mathbf{y}_{i}}}\n",
    "\\end{aligned}$\n",
    "\n",
    "\n",
    "We take log on both sides of the likelihood on both sides.\n",
    "\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\log[L(\\mathbf{w}|\\mathbf{y}, \\mathbf{X})] &= \\log[\\prod_{n=i}^{N} {\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})^{\\mathbf{y}_{i}}[1 - \\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})]^{1-\\mathbf{y}_{i}}}] \\\\\n",
    "&= \\sum_{n=i}^{N}\\mathbf{y}_{i}\\log(\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})) + \\big[1-\\mathbf{y}_{i}\\big]\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})\\big)\n",
    "\\end{aligned}$\n",
    "\n",
    "\n",
    "To want to maximize this likelihood, we can minimize the negative of the above expression and average it out, which we can define as cost function.\n",
    "$\\begin{aligned}\n",
    "\\mathbf{C}\\left(\\mathbf{w}\\right)= -\\frac{1}{\\mathbf{N}}\\Bigg[\\sum_{n=i}^{N}\\mathbf{y}_{i}\\log(\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})) + \\big[1-\\mathbf{y}_{i}\\big]\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})\\big)\\Bigg]\n",
    "\\end{aligned}$\n",
    "\n",
    "\n",
    "Let $\\mathbf{Z} = \\mathbf{w}^{T}\\mathbf{x}_{i}$.  We can write $\\mathbf{Z} = w_{0} + w_{1}x_{1} + w_{2}x_{2}$. Following from this equation, we see the following:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial Z}{\\partial w_0} = x_{i0} \\\\\n",
    "\\dfrac{\\partial Z}{\\partial w_1} = x_{i1} \\\\ \n",
    "\\dfrac{\\partial Z}{\\partial w_2} = x_{i2} \\\\ \\\\\n",
    "\\text{where}~x_{i0} &= \\text{a vector of 1s (an additional vector that we will later append to our X matrix)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence, we can write $\\dfrac{\\partial Z}{\\partial w_j} = x_{ij}$ <br>\n",
    "Now lets compute the derivative of $\\sigma\\left(\\mathbf{Z}\\right)$ which equals $\\frac{1}{1+e^{-z}}$.\n",
    "We use the quotient rule to do that and the following result. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial \\left(\\sigma \\left(Z\\right)\\right)}{\\partial w_{j}} &= \\frac{x_{ij} \\left(e^{-Z}\\right)}{\\left(1 + e^{-Z}\\right)^{2}} \\\\\n",
    "&= x_{ij}.\\frac{1}{1+e^{-Z}}.\\frac{e^{-Z}}{1+e^{-Z}} \\\\ \\\\\n",
    "&= x_{ij}.\\sigma\\left(Z\\right).\\left(1-\\sigma\\left(Z\\right)\\right) \\\\ \\\\\n",
    "\\text{where}~\\mathbf{Z} &= \\mathbf{w}^{T}\\mathbf{x}_{i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using the above equations, we calculate the $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$.\n",
    "$\\begin{aligned}\n",
    "\\mathbf{C}\\left(\\mathbf{w}\\right) &= -\\frac{1}{\\mathbf{N}}\\Bigg[\\sum_{n=i}^{N}\\mathbf{y}_{i}\\log(\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})) + \\big[1-\\mathbf{y}_{i}\\big]\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})\\big)\\Bigg] \\\\\n",
    "\\mathbf{C}\\left(\\mathbf{w}\\right) &=-\\frac{1}{\\mathbf{N}}\\sum_{n=i}^{N}f(\\mathbf{w}) + g(\\mathbf{w}) \\\\ \\\\\n",
    "\\text{where}~f(\\mathbf{w}) &= \\mathbf{y}_{i}\\log(\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})) \\; \\& \\; \\\\ g(\\mathbf{w}) &= \\big[1-\\mathbf{y}_{i}\\big]\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}_{i})\\big)\n",
    "\\end{aligned}$\n",
    "\n",
    "To make the differentiation simple, we start by differentiating $f(\\mathbf{w})$. We use the above shown derviates of $\\mathbf{w}^{T}\\mathbf{x}_{i}$ and $\\sigma\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)$ to do this.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial f(\\mathbf{w})}{\\partial \\mathbf{w_{j}}} &= y_{i}\\Big[\\frac{1}{\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}.\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).\\left(1 - \\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)\\right).x_{ij}\\Big] \\\\\n",
    "&= y_{i}.(1-\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)).x_{ij} \\\\\n",
    "&= y_{i}.x_{ij} - y_{i}.\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).x_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we differentiate $g(\\mathbf{w})$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial g(\\mathbf{w})}{\\partial \\mathbf{w_{j}}} &= 1- y_{i}\\Big[\\frac{1}{1- \\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}.\\left(1 - \\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)\\right).\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).-x_{ij}\\Big] \\\\\n",
    "&= \\left(1 - y_{i}\\right).\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).x_{ij} \\\\\n",
    "&= \\sigma\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).x_{ij} + y_{i}.\\sigma \\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right).x_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The simplified equation of $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\mathbf{w}}C(\\mathbf{w}) &=-\\frac{1}{N}\\sum_{i=1}^{N}\\Big[\\dfrac{\\partial f(\\mathbf{w})}{\\partial \\mathbf{w}} + \\dfrac{\\partial g(\\mathbf{w})}{\\partial \\mathbf{w}}\\Big] \\\\\n",
    "&= -\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The vector notation of $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$ is presented below:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = -\\frac{1}{N}\\Big[\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{i0},\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{i1},\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{i2}\\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_j^{(k+1)} &= w_j^{(k)} - \\eta\\Big[-\\frac{1}{N}\\sum_{i=1}^{N}\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{ij}\\Big] \\\\\n",
    "&= w_j^{(k)} + \\eta\\sum_{i=1}^{N}\\left(\\mathbf{y}_{i} - \\sigma{\\left(\\mathbf{w}^{T}\\mathbf{x}_{i}\\right)}\\right).x_{ij}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To minimize the cost function, we will be using the gradient descent rather than optimization methods. Below provided is the code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression class\n",
    "class Logistic_regression:\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self.w = None  # logistic regression weights\n",
    "        self.saved_w = []  # Since this is a small problem, we can save the weights\n",
    "        #  at each iteration of gradient descent to build our\n",
    "        #  learning curves\n",
    "        # returns nothing\n",
    "        pass\n",
    "\n",
    "    # Method for calculating the sigmoid function of w^T X for an input set of weights\n",
    "    def sigmoid(self, X, w):\n",
    "        X_T = X.transpose()\n",
    "        W_T_X = w @ X_T\n",
    "        numerator = 1\n",
    "        denominator = 1 + np.exp(-W_T_X)\n",
    "        return numerator / denominator\n",
    "\n",
    "    # Cost function for an input set of weights\n",
    "    def cost(self, X, y, w):\n",
    "        sig_WTX = self.sigmoid(X, w)\n",
    "        cost_i = (np.log(sig_WTX) * y) + ((1 - y) * np.log(1 - sig_WTX))\n",
    "        avg_cost = -1 * np.mean(cost_i)\n",
    "        return avg_cost\n",
    "\n",
    "    # Update the weights in an iteration of gradient descent\n",
    "    # added a w by ourselves\n",
    "    def gradient_descent(self, X, y, lr):\n",
    "        y_sig_WTX = y - self.sigmoid(X, self.w)\n",
    "        change_vector = (lr) * (y_sig_WTX @ X)\n",
    "        change_vector_norm = np.linalg.norm(change_vector)\n",
    "        return (change_vector_norm, change_vector)\n",
    "\n",
    "    # Fit the logistic regression model to the data through gradient descent\n",
    "    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n",
    "        self.w = w_init\n",
    "        self.saved_w.append(w_init)\n",
    "        iterations = 0\n",
    "        update_norm = 1  # place holder to initiate loop\n",
    "\n",
    "        while (iterations < max_iter) and (update_norm > delta_thresh):\n",
    "            update_norm, update_vect = self.gradient_descent(X, y, lr)\n",
    "            w_init = w_init + update_vect\n",
    "            self.w = w_init\n",
    "            self.saved_w.append(w_init)\n",
    "            iterations += 1\n",
    "            if verbose:\n",
    "                print(self.w)\n",
    "\n",
    "    # Use the trained model to predict the confidence scores\n",
    "    #  (prob of positive class in this case)\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X, self.w)\n",
    "        # returns the confidence score for the each sample\n",
    "\n",
    "    # Use the trained model to make binary predictions\n",
    "    def predict(self, X, thresh=0.5):\n",
    "        predicted_prob = self.sigmoid(X, self.w)\n",
    "        pred_class = np.where(predicted_prob > thresh, 1, 0)\n",
    "        return pred_class\n",
    "\n",
    "    # Stores the learning curves from saved weights from gradient descent\n",
    "    def learning_curve(self, X, y):\n",
    "        cost_all_iteration = []\n",
    "        for i in self.saved_w:\n",
    "            cost_each_iteration = self.cost(X, y, i)\n",
    "            cost_all_iteration.append(cost_each_iteration)\n",
    "\n",
    "        return cost_all_iteration\n",
    "\n",
    "    # Appends a column of ones as the first feature to account for the bias term\n",
    "    def prepare_x(self, X):\n",
    "        ones_vector = np.ones((X.shape[0], 1))\n",
    "        prepared_X = np.hstack((ones_vector, X))\n",
    "        return prepared_X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6744e8da450b552ad5b418e5f2a8e72a4bc784f77b3cd37656a8af206a4fedf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
